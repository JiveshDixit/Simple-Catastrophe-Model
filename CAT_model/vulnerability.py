# -*- coding: utf-8 -*-
# Copyright (c) 2024 Jivesh Dixit, P.S. II, NCMRWF
# All rights reserved.
#
# This software is licensed under MIT license.
# Contact [jdixit@nic.in; jiveshdixit@gmail.com]



###### catastrophe_model/vulnerability.py ######

import os
import logging
import gc
import pandas as pd
import geopandas as gpd
import concurrent.futures
import uuid
import osmnx as ox
import numpy as np
from utils import (
    split_polygon_into_grid,
    _fetch_buildings_for_polygon,
    optimize_data_types,
    sanitize_columns,
    save_to_geopackage
)


def get_osm_buildings(
    place_name: str,
    grid_size_km: int = 50,
    max_retries: int = 3,
    sleep_time: int = 5,
    parallel: bool = True,
    max_workers: int = 2,
    batch_size: int = 10
) -> gpd.GeoDataFrame:
    """
    Fetches building data from OSM for the specified place.
    Caches data in a GeoPackage for faster repeated runs.
    Processes data in batches to manage memory usage.
    """
    logging.basicConfig(filename='osm_fetching.log',
                        level=logging.WARNING,
                        format='%(asctime)s %(levelname)s:%(message)s')

    cache_filename = f'{place_name.replace(" ", "_")}_buildings.gpkg'  # Use place_name in filename


    if os.path.exists(cache_filename):
        print("Loading cached building data (GeoPackage)...")
        try:
            combined_buildings = gpd.read_file(cache_filename)
            print(f"Total buildings loaded from cache: {len(combined_buildings)}")
            return combined_buildings
        except Exception as e:
            print(f"Error reading cache file: {e}") 



    try:
        print(f"Fetching boundary for {place_name}...")
        gdf_place = ox.geocode_to_gdf(place_name)
        print("Boundary fetched.")
    except Exception as e:
        print(f"Error fetching boundary: {e}")
        return gpd.GeoDataFrame()


    print(f"Splitting '{place_name}' into {grid_size_km} km grid cells...")
    grid_clipped = split_polygon_into_grid(gdf_place, grid_size_km=grid_size_km)
    print(f"Total grids created: {len(grid_clipped)}")

    all_buildings = []
    total_grids = len(grid_clipped)
    print(f"Total grids to process: {total_grids}")


    for start in range(0, total_grids, batch_size):
        end = min(start + batch_size, total_grids)
        batch = grid_clipped.iloc[start:end]
        batch_num = start // batch_size + 1
        print(f"Processing batch {batch_num} (grid cells {start} to {end - 1})")

        if parallel:
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_idx = {
                    executor.submit(_fetch_buildings_for_polygon,
                                    row.geometry,
                                    idx,
                                    max_retries,
                                    sleep_time
                                    ): idx
                    for idx, row in batch.iterrows()
                }
                for future in concurrent.futures.as_completed(future_to_idx):
                    idx = future_to_idx[future]
                    try:
                        buildings = future.result()
                        if not buildings.empty:
                            buildings["Grid_Cell_ID"] = idx
                            all_buildings.append(buildings)
                    except Exception as e:
                        print(f"Grid Cell {idx} exception: {e}")
        else:
            for idx, row in batch.iterrows():
                buildings = _fetch_buildings_for_polygon(
                    row.geometry, 
                    idx, 
                    max_retries, 
                    sleep_time
                )
                if not buildings.empty:
                    buildings["Grid_Cell_ID"] = idx
                    all_buildings.append(buildings)


        if len(all_buildings) > 0:
            combined_batch = pd.concat(all_buildings, ignore_index=True)
            combined_batch = gpd.GeoDataFrame(combined_batch, crs="EPSG:4326")


            if "Type" in combined_batch.columns:
                combined_batch = combined_batch.drop(columns=["Type"])  # Or rename it

            batch_filename = f"{place_name.replace(' ', '_')}_buildings_batch_{batch_num}.gpkg"  # Use place_name
            save_to_geopackage(combined_batch, batch_filename, layer='buildings')

            print(f"Batch {batch_num} saved as '{batch_filename}'.")
            all_buildings = []
            gc.collect()


    combined_buildings = gpd.GeoDataFrame()
    for start in range(0, total_grids, batch_size):
        batch_num = start // batch_size + 1
        batch_file = f"{place_name.replace(' ', '_')}_buildings_batch_{batch_num}.gpkg"  # Use place_name
        if os.path.exists(batch_file):
            try:
                batch_gdf = gpd.read_file(batch_file)
                combined_buildings = pd.concat([combined_buildings, batch_gdf], ignore_index=True)
                print(f"Batch {batch_num} loaded from '{batch_file}'.")
            except Exception as e:
                print(f"Error reading batch file {batch_file}: {e}")
            finally:
                try:
                    os.remove(batch_file)
                    print(f"Batch file '{batch_file}' removed.")
                except Exception as e:
                    print(f"Error removing batch file {batch_file}: {e}")


    if not combined_buildings.empty:
        combined_buildings = gpd.GeoDataFrame(combined_buildings, crs="EPSG:4326")
        print(f"\nTotal buildings fetched: {len(combined_buildings)}")


        combined_buildings = optimize_data_types(combined_buildings)
        combined_buildings.reset_index(drop=True, inplace=True)
        if "Type" in combined_buildings.columns:

            combined_buildings.drop(columns=["Type"], inplace=True)


        if "building" in combined_buildings.columns and "Building_Type" in combined_buildings.columns:
            combined_buildings.drop(columns=["building"], inplace=True)
        elif "building" in combined_buildings.columns:
            combined_buildings.rename(columns={"building": "Building_Type"}, inplace=True)


        if "building_type" in combined_buildings.columns and "Building_Type" in combined_buildings.columns:

            combined_buildings.drop(columns=["building_type"], inplace=True)
        elif "building_type" in combined_buildings.columns:
            combined_buildings.rename(columns={"building_type": "Building_Type"}, inplace=True)


        if "Building_Type" not in combined_buildings.columns:
            combined_buildings["Building_Type"] = "other"


        if "osmid" in combined_buildings.columns:
            combined_buildings.rename(columns={"osmid": "Building_ID"}, inplace=True)
        elif "Building_ID" not in combined_buildings.columns:
            combined_buildings["Building_ID"] = [str(uuid.uuid4()) for _ in range(len(combined_buildings))]


        combined_buildings.dropna(subset=["geometry"], inplace=True)


        value_ranges = {
            'residential': (100000, 500000),
            'commercial': (200000, 1000000),
            'industrial': (300000, 1500000),
            'retail': (150000, 800000),
            'office': (200000, 1000000),
            'other': (100000, 700000)
        }
        def assign_asset_value(btype):
            btype_l = str(btype).lower()
            val_range = value_ranges.get(btype_l, value_ranges['other'])
            return np.random.uniform(val_range[0], val_range[1])

        combined_buildings['Value_USD'] = combined_buildings['Building_Type'] \
            .apply(assign_asset_value) \
            .astype('float32')


        combined_proj = combined_buildings.to_crs(epsg=32614)
        combined_proj["centroid"] = combined_proj.geometry.centroid
        centroids_geo = combined_proj.set_geometry("centroid").to_crs(epsg=4326)
        combined_buildings["Latitude"] = centroids_geo["centroid"].y.astype('float32')
        combined_buildings["Longitude"] = centroids_geo["centroid"].x.astype('float32')


        combined_buildings = sanitize_columns(combined_buildings)


        combined_buildings.to_file(
            cache_filename,
            driver='GPKG',
            encoding='UTF-8',
            layer='buildings'
        )
        print(f"Building data saved to '{cache_filename}'.")
    else:

        combined_buildings = gpd.GeoDataFrame(
            columns=['Building_ID', 'Building_Type', 'geometry', 
                     'Latitude', 'Longitude', 'Value_USD']
        )
        print("No buildings fetched.")

    gc.collect()
    return combined_buildings